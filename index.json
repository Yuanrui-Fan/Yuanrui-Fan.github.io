[{"authors":["admin"],"categories":null,"content":"I am is currently a PhD student supervised by Prof. Shanping Li in the College of Computer Science and Technology, Zhejiang University, China. I am also supervised by Dr. Xin Xia. My research interests include mining software repositories, empirical software engineering and software analytics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://Yuanrui-Fan.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am is currently a PhD student supervised by Prof. Shanping Li in the College of Computer Science and Technology, Zhejiang University, China. I am also supervised by Dr. Xin Xia. My research interests include mining software repositories, empirical software engineering and software analytics.","tags":null,"title":"Yuanrui Fan","type":"authors"},{"authors":["**Yuanrui Fan**","Xin Xia","Daniel Alencar da Costa","David Lo","Ahmed E. Hassan","Shanping Li"],"categories":null,"content":"","date":1563408000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563408000,"objectID":"ccd765a2c05cf5a46f2546d866a2031a","permalink":"https://Yuanrui-Fan.github.io/publication/tse2019/","publishdate":"2019-07-18T00:00:00Z","relpermalink":"/publication/tse2019/","section":"publication","summary":"In this paper, we investigate the impact of the mislabeled changes by different SZZ variants on the performance and interpretation of JIT defect prediction models. We analyze four SZZ variants (i.e., B-SZZ, AG-SZZ, MA-SZZ, and RA-SZZ) that are proposed by prior studies. We build the prediction models using the labeled data by these four SZZ variants. Among the four SZZ variants, RA-SZZ is least likely to generate mislabeled changes, and we construct the testing set by using RA-SZZ. All of the four prediction models are then evaluated on the same testing set. We choose the prediction model built on the labeled data by RA-SZZ as the baseline model, and we compare the performance and metric importance of the models trained using the labeled data by the other three SZZ variants with the baseline model. Through a large-scale empirical study on a total of 126,526 changes from ten Apache open source projects, we find that in terms of various performance measures (AUC, F1-score, G-mean and Recall@20%), the mislabeled changes by B-SZZ and MA-SZZ are not likely to cause a considerable performance reduction, while the mislabeled changes by AG-SZZ cause a statistically significant performance reduction with an average difference of 1%–5%. When considering developers’ inspection effort (measured by LOC) in practice, the changes mislabeled B-SZZ and AG-SZZ lead to 9%–10% and 1%–15% more wasted inspection effort, respectively. And the mislabeled changes by B-SZZ lead to significantly more wasted effort. The mislabeled changes by MA-SZZ do not cause considerably more wasted effort. We also find that the top-most important metric for identifying bug-introducing changes (i.e., number of files modified in a change) is robust to the mislabeling noise generated by SZZ. But the second- and third-most important metrics are more likely to be impacted by the mislabeling noise, unless random forest is used as the underlying classifier.","tags":null,"title":"The Impact of Mislabeled Changes by SZZ on Just-in-Time Defect Prediction","type":"publication"},{"authors":["Liang Cai","**Yuanrui Fan**","Meng Yan","Xin Xia"],"categories":null,"content":"","date":1557446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557446400,"objectID":"39ba6944ed62be5ebacdac95cd0b5e5b","permalink":"https://Yuanrui-Fan.github.io/publication/jos2019/","publishdate":"2019-05-10T00:00:00Z","relpermalink":"/publication/jos2019/","section":"publication","summary":"Software defect prediction is always one of the most active research areas in software engineering. Researchers have proposed a lot of defect prediction techniques. These techniques consist of module-level, file-level, and change-level defect prediction according to the granularity. Change-level defect prediction can predict the defect-proneness of changes when they are initially submitted. Hence, such a technique is referred to as just-in-time defect prediction. Recently, just-in-time defect prediction becomes the hot area in defect prediction because of its timely manner and fine-grained. There are a lot of achievements in this area and there are also many challenges in data labeling, feature extracting, and model evaluation. More advanced and unified theoretic and technical guidelines are needed to enhance just-in-time defect prediction. Therefore, in this study, a road map for prior just-in-time defect prediction studies is presented in three folds, data labeling, feature extraction, and model evaluation. In summary, the contributions of this study are: (1) The data labeling methods and their advantages and disadvantages are concluded; (2) The feature categories and computing methods are concluded and classified; (3) The modeling techniques are concluded and classified; (4) The model validation and performance measures in model evaluation are concluded; (5) The current problems in this area are highlighted; and (6) The trends of Just-in-Time defect prediction are concluded.","tags":null,"title":"即时软件缺陷预测研究进展 Just-in-time Software Defect Prediction: Literature Review","type":"publication"},{"authors":["**Yuanrui Fan**","Xin Xia","David Lo","Ahmed E. Hassan"],"categories":null,"content":"","date":1533600000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533600000,"objectID":"c9578431d1ec11a309d1c411fe4d1ef6","permalink":"https://Yuanrui-Fan.github.io/publication/tse2018/","publishdate":"2018-08-07T00:00:00Z","relpermalink":"/publication/tse2018/","section":"publication","summary":"In this study, we propose an approach which can determine whether a newly submitted bug report is valid. Our approach first extracts 33 features from bug reports. The extracted features are grouped along 5 dimensions, i.e., reporter experience, collaboration network, completeness, readability and text. Based on these features, we use a random forest classifier to identify valid bug reports. To evaluate the effectiveness of our approach, we experiment on large-scale datasets containing a total of 560,697 bug reports from five open source projects (i.e., Eclipse, Netbeans, Mozilla, Firefox and Thunderbird). On average, across the five datasets, our approach achieves an F1-score for valid bug reports and F1-score for invalid ones of 0.74 and 0.67, respectively. Moreover, our approach achieves an average AUC of 0.81. In terms of AUC and F1-scores for valid and invalid bug reports, our approach statistically significantly outperforms two baselines using features that are proposed by Zanetti et al. [99]. We also study the most important features that distinguish valid bug reports from invalid ones. We find that the textual features of a bug report and reporter’s experience are the most important factors to distinguish valid bug reports from invalid ones.","tags":null,"title":"Chaff from the Wheat: Characterizing and Determining Valid Bug Reports","type":"publication"},{"authors":["**Yuanrui Fan**","Xin Xia","David Lo","Shanping Li"],"categories":null,"content":"","date":1521417600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521417600,"objectID":"94359b3ebdf9eccd3f69b7dc37e6cf7c","permalink":"https://Yuanrui-Fan.github.io/publication/emse2018/","publishdate":"2018-03-19T00:00:00Z","relpermalink":"/publication/emse2018/","section":"publication","summary":"Modern Code Review (MCR) has been widely used by open source and proprietary software projects. Inspecting code changes consumes reviewers much time and effort since they need to comprehend patches, and many reviewers are often assigned to review many code changes. Note that a code change might be eventually abandoned, which causes waste of time and effort. Thus, a tool that predicts early on whether a code change will be merged can help developers prioritize changes to inspect, accomplish more things given tight schedule, and not waste reviewing effort on low quality changes. In this paper, motivated by the above needs, we build a merged code change prediction tool. Our approach first extracts 34 features from code changes, which are grouped into 5 dimensions: code, file history, owner experience, collaboration network, and text. And then we leverage machine learning techniques such as random forest to build a prediction model. To evaluate the performance of our approach, we conduct experiments on three open source projects (i.e., Eclipse, LibreOffice, and OpenStack), containing a total of 166,215 code changes. Across three datasets, our approach statistically significantly improves random guess classifiers and two prediction models proposed by Jeong et al. (2009) and Gousios et al. (2014) in terms of several evaluation metrics. Besides, we also study the important features which distinguish merged code changes from abandoned ones.","tags":null,"title":"Early prediction of merged code changes to prioritize reviewing tasks","type":"publication"}]